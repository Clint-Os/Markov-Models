{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0145a19a",
   "metadata": {},
   "source": [
    "# 03 – Estimation (MLE with Monte-Carlo marginalisation & Stan skeleton)\n",
    "\n",
    "In this notebook, we fit the mixed hidden Markov model to a simulated dataset\n",
    "(`data/simulated/ref_scenario.csv`) generated in `02_simulator.ipynb`.\n",
    "\n",
    "It contains:\n",
    "- A Monte-Carlo marginalization approach for MLE (approximate marginal likelihood)\n",
    "- A CmdStanPy / Stan model skeleton for Bayesian inference (marginalize discrete states via forward algorithm)\n",
    "\n",
    "**Notes:**\n",
    "- MLE via MC is computationally heavy (many forward calls); we provide a small quick test and an option to scale up.\n",
    "- The Stan skeleton is illustrative and requires more tuning & priors to run for production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01c2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from models.emissions import EmissionModel\n",
    "from models.transitions import TransitionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65955d49",
   "metadata": {},
   "source": [
    "Stable log-sum-exp and forward using EmissionModel.logpdf  (check eqtn 11 in the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f8baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "def logsumexp_arr(a):\n",
    "    m = np.max(a)\n",
    "    return m + np.log(np.sum(np.exp(a-m)))\n",
    "\n",
    "def forward_loglik_subject(obs, times, init_probs, trans_mat, emission_model, g):\n",
    "\n",
    "    \"\"\"Compute the log-likelihood of a single subject given fixed\n",
    "    individual effects g using the forward algorithm in log-space for numerical stability.\n",
    "    Args:\n",
    "        obs (np.ndarray): Observations for the subject (T x D).\n",
    "        times (np.ndarray): Observation times for the subject (T,).\n",
    "        g: dict of random effects for the subject.\n",
    "        \"\"\"\n",
    "    T = len(obs)\n",
    "    n_states = 2\n",
    "    #initialize log alpha\n",
    "    log_alpha = np.zeros((T, n_states)) # log forward probabilities\n",
    "\n",
    "    for s in range(n_states):\n",
    "        log_em = emission_model.logpdf(obs[0], g, times[0], s)\n",
    "        log_alpha[0, s] = np.log(init_probs[s] + EPS) + log_em \n",
    "\n",
    "    #normalize in log-space by subtracting logsumexp\n",
    "    for t in range(1, T): #forward recursion\n",
    "        new_log_alpha = np.full(n_states, -np.inf)\n",
    "        for j in range(n_states):\n",
    "            #compute logsum over previous states\n",
    "            prev = log_alpha[t-1, :] + np.log(trans_mat[:, j] + EPS) \n",
    "            s_prev = logsumexp_arr(prev)\n",
    "            log_em = emission_model.logpdf(obs[t], g, times[t], j)\n",
    "            new_log_alpha[j] = s_prev + log_em #combine transition and emission log-probs\n",
    "        log_alpha[t, :] = new_log_alpha \n",
    "    \n",
    "    return logsumexp_arr(log_alpha[-1, :]) #marginal log-likelihood is logsumexp of final alphas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99390563",
   "metadata": {},
   "source": [
    "Load simulated data and prep per-subject datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b70ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/simulated/ref_scenario.csv'\n",
    "assert os.path.exists(data_path), f\"Data file not found at {data_path}. Run 02_simulator.ipynb to generate it.\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac177910",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects = df['ID'].unique()\n",
    "subj_data = {}\n",
    "\n",
    "for sid in subjects:\n",
    "    sub = df[df.ID ==sid].sort_values(\"Week\")\n",
    "    obs = sub[[\"FEV1\",\"PRO\"]].values\n",
    "    times = sub[\"Week\"].values.astype(float)\n",
    "    subj_data[int(sid)] = {'obs': obs, 'times': times}\n",
    "\n",
    "len(subj_data), list(subj_data.keys())[:5] # Check number of subjects and first 5 IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156e064",
   "metadata": {},
   "source": [
    "Monte_Carlo marginal log-likelihood per subject\n",
    "\n",
    "-For each candidate population parameter set, for each subject we:\n",
    "\n",
    "-> draw K samples of g ~ N(0, x2_*) using the candidate x2 variances,\n",
    "\n",
    "-> compute forward log-likelihood for each g,\n",
    "\n",
    "-> average the likelihoods (in log-space using log-sum-exp) to approximate the marginal likelihood.\n",
    "\n",
    "Since this is slow for many subjects / large K, it's advisable to use small tests (e.g., K=30) before scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e6df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_g_matrix_em_params(n_samples, rng, x2_params):\n",
    "    \"\"\"\n",
    "    Draw n_samples of g according to x2 variances dict.\n",
    "    x2_params: dict with keys ['x2_FEV1R','x2_FEV1E','x2_PROR','x2_PROE']\n",
    "    Returns list of dicts length n_samples.\n",
    "    \"\"\"\n",
    "    s_FEv1R = np.sqrt(x2_params['x2_FEV1R'])  #standard deviations\n",
    "    s_FEV1E = np.sqrt(x2_params['x2_FEV1E'])\n",
    "    s_PROR = np.sqrt(x2_params['x2_PROR'])\n",
    "    s_PROE = np.sqrt(x2_params['x2_PROE'])\n",
    "    rng = np.random.default_rng(rng)\n",
    "    gs = []\n",
    "    for _ in range(n_samples): #draw n_samples\n",
    "        g = {\n",
    "            \"gFEV1R\": float(rng.normal(0.0, s_FEv1R)),\n",
    "            \"gFEV1E\": float(rng.normal(0.0, s_FEV1E)),\n",
    "            \"gPROR\":  float(rng.normal(0.0, s_PROR)),\n",
    "            \"gPROE\":  float(rng.normal(0.0, s_PROE))\n",
    "        }\n",
    "        gs.append(g)\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f3d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_subject_log_marginal(obs, times, init_probs, trans_mat, em_params, K=30, rng_seed=None):\n",
    "    \"\"\"Approximate the marginal log-likelihood of a subject\n",
    "    by Monte-Carlo integration over g.\n",
    "    Args:\n",
    "        obs (np.ndarray): Observations for the subject (T x D).\n",
    "        times (np.ndarray): Observation times for the subject (T,).\n",
    "        em_params: dict of emission parameters including variance components.\n",
    "        K (int): Number of Monte-Carlo samples.\"\"\"\n",
    "\n",
    "    #build emission model from em_params\n",
    "    em = EmissionModel(\n",
    "        hFEV1R=em_params['hFEV1R'],\n",
    "        hFEV1E=em_params['hFEV1E'],\n",
    "        x2_FEV1R=em_params['x2_FEV1R'],\n",
    "        x2_FEV1E=em_params['x2_FEV1E'],\n",
    "        hPROR=em_params['hPROR'],\n",
    "        hPROE=em_params['hPROE'],\n",
    "        x2_PROR=em_params['x2_PROR'],\n",
    "        x2_PROE=em_params['x2_PROE'],\n",
    "        r2_FEV1=em_params['r2_FEV1'],\n",
    "        r2_PRO=em_params['r2_PRO'],\n",
    "        qR=em_params['qR'],\n",
    "        qE=em_params['qE'],\n",
    "        PE=em_params['PE'],\n",
    "        PHL=em_params['PHL']\n",
    "    )\n",
    "\n",
    "    #sample K g's\n",
    "    x2_params = {k: em_params[k] for k in ['x2_FEV1R','x2_FEV1E','x2_PROR','x2_PROE']} \n",
    "    gs = sample_g_matrix_em_params(K, rng_seed, x2_params)\n",
    "    #compute log-likelihoods for each g\n",
    "    logls = []\n",
    "    for g in gs:\n",
    "        logl_g = forward_loglik_subject(obs, times, init_probs, trans_mat, em, g)\n",
    "        logls.append(logl_g)\n",
    "    #combine via log-mean-exp: log(1/k * sum exp(logl_g ))\n",
    "    logls = np.array(logls)\n",
    "    lm = logsumexp_arr(logls) - np.log(K)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7db573",
   "metadata": {},
   "source": [
    "## Full data negative log-likelihood to optimize\n",
    "\n",
    "We assemble a function neg_loglik(params) that maps a parameter vector to the negative log-likelihood across subjects using the Monte Carlo approximation.\n",
    "For clarity we will optimize a reduced parameter set in this notebook\n",
    "\n",
    "Examples of parameters we estimate here:\n",
    "\n",
    "-> hFEV1R, hFEV1E, hPROR, hPROE (modes)\n",
    "\n",
    "-> log of IIV variances: log_x2_FEV1R, log_x2_FEV1E, log_x2_PROR, log_x2_PROE\n",
    "\n",
    "-> log of residual variances: log_r2_FEV1, log_r2_PRO\n",
    "\n",
    "-> Fisher-transformed correlations btn the variables: atanh(qR), atanh(qE)\n",
    "\n",
    "-> hpRE and hpER (we optimize them on logit scale)\n",
    "\n",
    "For a quick test we only fit a subset (e.g., hFEV1R, hPROR, log residuals). The fixed_mask is for fixing parameters if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971090a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def logit(p):\n",
    "    p = np.clip(p, EPS, 1.0 - EPS)\n",
    "    return np.log(p / (1.0 - p))\n",
    "# Parameter indexing helpers (order)\n",
    "param_names = [\n",
    "    'hFEV1R','hFEV1E','hPROR','hPROE',\n",
    "    'log_x2_FEV1R','log_x2_FEV1E','log_x2_PROR','log_x2_PROE',\n",
    "    'log_r2_FEV1','log_r2_PRO',\n",
    "    'atanh_qR','atanh_qE',\n",
    "    'logit_hpRE','logit_hpER'\n",
    "]\n",
    "\n",
    "def pack_params_from_vector(vec):\n",
    "    \"\"\"\n",
    "    Convert unconstrained vector to em_params dict and transition params.\n",
    "    `vec` must follow the param_names order.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    idx = 0\n",
    "    d['hFEV1R'] = vec[idx]; idx+=1  #must be positive to \n",
    "    d['hFEV1E'] = vec[idx]; idx+=1\n",
    "    d['hPROR']  = vec[idx]; idx+=1\n",
    "    d['hPROE']  = vec[idx]; idx+=1\n",
    "\n",
    "    d['x2_FEV1R'] = np.exp(vec[idx]); idx+=1\n",
    "    d['x2_FEV1E'] = np.exp(vec[idx]); idx+=1\n",
    "    d['x2_PROR']  = np.exp(vec[idx]); idx+=1\n",
    "    d['x2_PROE']  = np.exp(vec[idx]); idx+=1\n",
    "\n",
    "    d['r2_FEV1'] = np.exp(vec[idx]); idx+=1\n",
    "    d['r2_PRO']  = np.exp(vec[idx]); idx+=1\n",
    "\n",
    "    d['qR'] = np.tanh(vec[idx]); idx+=1\n",
    "    d['qE'] = np.tanh(vec[idx]); idx+=1\n",
    "\n",
    "    d['hpRE'] = logistic(vec[idx]); idx+=1\n",
    "    d['hpER'] = logistic(vec[idx]); idx+=1\n",
    "\n",
    "    # placeholders for PE, PHL (fixed)\n",
    "    d['PE'] = 0.2\n",
    "    d['PHL'] = 10.0\n",
    "    return d\n",
    "\n",
    "# default initial vector set to values close to simulation\n",
    "init_vec = np.array([\n",
    "    3.0, 0.5,  # hFEV1R, hFEV1E\n",
    "    2.5, 0.5,  # hPROR, hPROE\n",
    "    np.log(0.03), np.log(0.03), np.log(0.09), np.log(0.09),  # log x2\n",
    "    np.log(0.015), np.log(0.05),  # log r2\n",
    "    np.arctanh(-0.33), np.arctanh(-0.33),  # atanh q\n",
    "    logit(0.1), logit(0.3)  # logit hpRE, hpER\n",
    "])\n",
    "\n",
    "# Choose a subset to optimize for a quick demo (e.g., first 10 params)\n",
    "free_idx = list(range(len(init_vec)))  # by default free all\n",
    "# for a faster demo you might limit:\n",
    "# free_idx = list(range(10))  # fit only the first 10 parameters\n",
    "\n",
    "def neg_loglik_mc(vec, subj_data_local, init_probs, trans_mat_fn, K=30, rng_seed=42):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood across all subjects approximated by MC integration.\n",
    "    trans_mat_fn: function that given em_params returns transition matrix (we keep hpRE/hpER in params)\n",
    "    \"\"\"\n",
    "    em_params = pack_params_from_vector(vec)\n",
    "    # build trans_mat from hpRE/hpER (no gpRE/gpER here)\n",
    "    tm = TransitionModel(hpRE=em_params['hpRE'], hpER=em_params['hpER'])\n",
    "    trans_mat = tm.transition_matrix()\n",
    "\n",
    "    total_logl = 0.0\n",
    "    rng_seq = np.random.SeedSequence(rng_seed)\n",
    "    # Loop subjects (consider batching / parallelization later)\n",
    "    for i, sid in enumerate(subj_data_local.keys()):\n",
    "        d = subj_data_local[sid]\n",
    "        lm = approx_subject_log_marginal(\n",
    "            d['obs'], d['times'],\n",
    "            init_probs, trans_mat, em_params, K=K, rng_seed=rng_seq.spawn(1)[0].entropy\n",
    "        )\n",
    "        total_logl += lm\n",
    "    return -float(total_logl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc46b11",
   "metadata": {},
   "source": [
    "Quick test of the fn neg_loglik with a small subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df31961",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_subjects = {k: subj_data[k] for k in list(subj_data.keys())[:5]}  # first 5 subjects for quick test\n",
    "init_probs = np.array([0.9, 0.1])  \n",
    "\n",
    "t0 = time.time()\n",
    "nl = neg_loglik_mc(init_vec, test_subjects, init_probs, None, K=20, rng_seed=1)\n",
    "t1 = time.time()\n",
    "print(f\"Neg log-likelihood (5 subjects, K=20): {nl:.3f} computed in {t1 - t0:.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d9733",
   "metadata": {},
   "source": [
    "# Optimization demo\n",
    "\n",
    "-> We minimize the negative log-likelihood of our mHMM model to find the params that maximizie the likelihood of our observed data.\n",
    "\n",
    "-> We use L-BFGS-B algorithm - gradient-based optimization algorithm that efficiently handles large numbers of parameters and parameter bounds (e.g. variances > 0). More stable than basic gradient-descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565cd611",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_subset = {k: subj_data[k] for k in list(subj_data.keys())[10]}  \n",
    "\n",
    "#show progress \n",
    "history = []\n",
    "def cb(xk):\n",
    "    history.append(xk.copy())\n",
    "    print('.', end='') \n",
    "\n",
    "res = minimize(\n",
    "    fun = lambda v: neg_loglik_mc(v, subject_subset, init_probs, None, K=30, rng_seed=1),\n",
    "    x0=init_vec,\n",
    "    method='L-BFGS-B',\n",
    "    options={'disp': True, 'maxiter': 50}\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization result:\")\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecea832",
   "metadata": {},
   "source": [
    "# Interpret & decode Viterbi using a point estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7979dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if res.success:\n",
    "    fitted_params = pack_params_from_vector(res.x)\n",
    "    tm_hat = TransitionModel(hpRE=fitted_params['hpRE'], hpER=fitted_params['hpER'])\n",
    "    trans_mat_hat = tm_hat.transition_matrix()\n",
    "    em_hat = EmissionModel(\n",
    "        hFEV1R=fitted_params['hFEV1R'], hFEV1E=fitted_params['hFEV1E'],\n",
    "        x2_FEV1R=fitted_params['x2_FEV1R'], x2_FEV1E=fitted_params['x2_FEV1E'],\n",
    "        hPROR=fitted_params['hPROR'], hPROE=fitted_params['hPROE'],\n",
    "        x2_PROR=fitted_params['x2_PROR'], x2_PROE=fitted_params['x2_PROE'],\n",
    "        r2_FEV1=fitted_params['r2_FEV1'], r2_PRO=fitted_params['r2_PRO'],\n",
    "        qR=fitted_params['qR'], qE=fitted_params['qE'],\n",
    "        PE=fitted_params['PE'], PHL=fitted_params['PHL']\n",
    "    )\n",
    "\n",
    "    from models.mhmm_forward import viterbi  # or we can use the viterbi function from 01_model_math (conv to .py file)\n",
    "    # Naive g=0\n",
    "    g0 = {\"gFEV1R\":0.0,\"gFEV1E\":0.0,\"gPROR\":0.0,\"gPROE\":0.0}\n",
    "    sample_sid = list(subject_subset.keys())[0]\n",
    "    sdata = subject_subset[sample_sid]\n",
    "    states_decoded = viterbi(sdata['obs'], np.array([0.9,0.1]), trans_mat_hat, em_hat, g0, sdata['times'])\n",
    "    print(\"Decoded states (naive g=0):\", states_decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce68e338",
   "metadata": {},
   "source": [
    "# Stan Data Builder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad9838",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/simulated/ref_scenario.csv'\n",
    "assert os.path.exists(data_path), f\"Data file not found at {data_path}. Run 02_simulator.ipynb to generate it.\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "subjects = sorted(df['ID'].unique())\n",
    "N = len(subjects)\n",
    "\n",
    "y1_flat, y2_flat, time_flat= [], [], []\n",
    "subj_start, subj_len, trt_slp = [], [], []\n",
    "\n",
    "pos = 0 #position in flat arrays\n",
    "for sid in subjects:\n",
    "    sub = df[df.ID == sid].sort_values(\"Week\")\n",
    "    subj_start.append(pos + 1)  #1-based index for Stan \n",
    "    L = sub.shape[0]\n",
    "    subj_len.append(L)\n",
    "    pos += L\n",
    "\n",
    "    y1_flat.extend(sub['FEV1'].values.tolist())\n",
    "    y2_flat.extend(sub['PRO'].values.tolist())\n",
    "    time_flat.extend(sub['Week'].values.tolist())\n",
    "\n",
    "    trt_slp.append(0.0) \n",
    "\n",
    "stan_data = {\n",
    "    'N': N,\n",
    "    'T_max': max(subj_len),\n",
    "    'total_obs': len(y1_flat),\n",
    "    'y1': y1_flat,\n",
    "    'y2': y2_flat,\n",
    "    'time_flat': time_flat,\n",
    "    'subj_start': subj_start,\n",
    "    'subj_len': subj_len,\n",
    "    'trt_slp': trt_slp,\n",
    "    'init_prob': [0.9, 0.1]\n",
    "}\n",
    "\n",
    "print(f'Stan data built for {N} subject, total{len(y1_flat)} observations.')\n",
    "print(f'Example: subj_len[:5] = {'subj_len[:5]}'}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0b92d",
   "metadata": {},
   "source": [
    "Run CmdStanPy Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd0c87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmdstanpy import CmdStanModel\n",
    "import time, pickle \n",
    "\n",
    "stan_file = '../models/stan/mhmm_marginal.stan'\n",
    "\n",
    "assert os.path.exists(stan_file), f\"Stan model file not found at {stan_file}. Check models/stan directory.\"\n",
    "\n",
    "model = CmdStanModel(stan_file=stan_file)\n",
    "\n",
    "#Run CmdStanPy Sampling\n",
    "start = time.time()\n",
    "fit = model.sample(\n",
    "    data=stan_data,\n",
    "    seed=123,\n",
    "    chains=4,\n",
    "    parallel_chains=4,\n",
    "    iter_warmup=1000,\n",
    "    iter_sampling=1000,\n",
    "    adapt_delta=0.85,\n",
    "    show_progress=True \n",
    ")\n",
    "end = time.time()\n",
    "print(f'Sampling completed in {end - start/60:.2f} minutes.')\n",
    "\n",
    "os.makedirs('../data/results', exist_ok=True)\n",
    "fit.save_csvfiles(dir='../data/results')\n",
    "with open('../data/results/mhmm_stan_fit.pkl', 'wb') as f:\n",
    "    pickle.dump(fit, f)\n",
    "\n",
    "print(\"Stan fit results saved.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848c62e",
   "metadata": {},
   "source": [
    "# Posterior Summaries and Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097a73d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az \n",
    "\n",
    "fit_path = '../data/results/mhmm_stan_fit.pkl'\n",
    "assert os.path.exists(fit_path), f\"Fit file not found at {fit_path}. Load the fit results first.\"\n",
    "\n",
    "with open(fit_path, 'rb') as f:\n",
    "    fit = pickle.load(f)\n",
    "\n",
    "idata = az.from_cmdstanpy(posterior=fit)\n",
    "\n",
    "params_of_interest = [\n",
    "    'hFEV1R','hFEV1E','hPROR','hPROE',\n",
    "    'r2_FEV1','r2_PRO',\n",
    "    'qR','qE', 'hpRE','hpER',\n",
    "    'PE','PHL'] \n",
    "\n",
    "#we convert transformed params back to original scale for variances\n",
    "post = fit.draws_pd() \n",
    "post['hpRE'] = 1 / (1 + np.exp(-post['logit_hpRE']))\n",
    "post['hpER'] = 1 / (1 + np.exp(-post['logit_hpER']))\n",
    "post['qR'] = np.tanh(post['atanh_qR'])\n",
    "post['qE'] = np.tanh(post['atanh_qE'])\n",
    "\n",
    "summary = post[params_of_interest].describe(percentiles=[0.025, 0.5, 0.975]).T\n",
    "summary = summary[['mean', 'std', '2.5%', '50%', '97.5%']]\n",
    "print(\"Posterior summary (mean +-SD, 95% CrI):\")\n",
    "display(summary.round(3))\n",
    "\n",
    "\n",
    "#convergence diagnostics\n",
    "az.plot_trace(idata, var_names=['hFEV1R','hPROR','r2_FEV1','hpRE'], compact=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#compare to reference values\n",
    "ref_params = {\n",
    "    'hFEV1R': 3.0,\n",
    "    'hFEV1E': 0.5,\n",
    "    'hPROR': 2.5,\n",
    "    'hPROE': 0.5,\n",
    "    'r2_FEV1': 0.015,\n",
    "    'r2_PRO': 0.05,\n",
    "    'qR': -0.33,\n",
    "    'qE': -0.33,\n",
    "    'hpRE': 0.1,\n",
    "    'hpER': 0.3,\n",
    "    'PE': 0.2,\n",
    "    'PHL': 10.0\n",
    "}\n",
    "\n",
    "diffs = {}\n",
    "for k in ref_params.keys():\n",
    "    if k in summary.index:\n",
    "        diffs[k] = summary.loc[k, 'mean'] - ref_params[k]\n",
    "\n",
    "pd.Series(diffs).round(3).to_frame('Posterior - Reference').style_background_gradient(cmap='RdYlBu_r') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f342f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime \n",
    "\n",
    "# Ensure output folder exists\n",
    "summary_dir = \"../data/results/summary\"\n",
    "os.makedirs(summary_dir, exist_ok=True) \n",
    "\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "summary_csv = os.path.join(summary_dir, f\"posterior_summary_{timestamp}.csv\")\n",
    "trace_png   = os.path.join(summary_dir, f\"traceplot_{timestamp}.png\")\n",
    "posterior_png = os.path.join(summary_dir, f\"posterior_density_{timestamp}.png\")\n",
    "diffs_csv   = os.path.join(summary_dir, f\"posterior_minus_ref_{timestamp}.csv\")\n",
    "\n",
    "# Save posterior summary table\n",
    "summary.round(4).to_csv(summary_csv)\n",
    "print(f\"Posterior summary saved to {summary_csv}\")\n",
    "\n",
    "\n",
    "fig_trace = az.plot_trace(idata, var_names=[\"hFEV1R\", \"hPROR\", \"r2_FEV1\", \"hpRE\"], compact=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(trace_png, dpi=300)\n",
    "plt.close()\n",
    "print(f\"Traceplot saved to {trace_png}\")\n",
    "\n",
    "fig_post = az.plot_posterior(idata, var_names=[\"hFEV1R\", \"hPROR\", \"hpRE\", \"hpER\"], hdi_prob=0.95)\n",
    "plt.tight_layout()\n",
    "plt.savefig(posterior_png, dpi=300)\n",
    "plt.close()\n",
    "print(f\"Posterior density plot saved to {posterior_png}\")\n",
    "\n",
    "# Save difference (Posterior − Reference) table\n",
    "pd.Series(diffs).round(4).to_csv(diffs_csv)\n",
    "print(f\"Differences vs reference saved to {diffs_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf57ae6",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "- In this notebook we have:\n",
    "\n",
    "1. Implemented two estimation approaches for the mixed Hidden Markov Model (mHMM):\n",
    "   - **Monte Carlo–based Maximum Likelihood Estimation (MLE)** for quick testing and validation.\n",
    "   - **Bayesian estimation in Stan (CmdStanPy)** as an open-source alternative to NONMEM + SAEM.\n",
    "\n",
    "2. Utilized the Stan model which reproduces the same likelihood structure as the paper:\n",
    "   - Population modes (`h*`), individual random effects (`g`), and residual variances (`r2_*`).\n",
    "   - State-specific correlations and time-dependent placebo term (`PE`, `PHL`).\n",
    "   - Transition probabilities following logistic forms from Eqs. 7–10.\n",
    "\n",
    "3. We flattened the simulated data from `02_simulator.ipynb` and formatted for Stan’s input requirements.\n",
    "\n",
    "4. Provided code to summarize sampling results, visualize (trace and posterior densities), and compare to the reference scenario (Table 1 in the paper).\n",
    "\n",
    "5. Exported all posterior summaries and diagnostics to `data/results/summary/` for reproducibility and later use in the simulation–estimation evaluation (SSE) stage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ce53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f44c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhmm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
