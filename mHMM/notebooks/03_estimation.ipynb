{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0145a19a",
   "metadata": {},
   "source": [
    "# 03 ‚Äì Estimation (MLE with Monte-Carlo marginalisation & Stan skeleton)\n",
    "\n",
    "In this notebook, we fit the mixed hidden Markov model to a simulated dataset\n",
    "(`data/simulated/ref_scenario.csv`) generated in `02_simulator.ipynb`.\n",
    "\n",
    "It contains:\n",
    "- A Monte-Carlo marginalization approach for MLE (approximate marginal likelihood)\n",
    "- A CmdStanPy / Stan model skeleton for Bayesian inference (marginalize discrete states via forward algorithm)\n",
    "\n",
    "**Notes:**\n",
    "- MLE via MC is computationally heavy (many forward calls); we provide a small quick test and an option to scale up.\n",
    "- The Stan skeleton is illustrative and requires more tuning & priors to run for production.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65932d58-4b12-4c37-8223-95f5a3c28ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/user/Documents/Projects/Markov-Models/mHMM\n",
      "src path: /Users/user/Documents/Projects/Markov-Models/mHMM/src\n",
      "utils path: /Users/user/Documents/Projects/Markov-Models/mHMM/utils\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "src_path = os.path.join(project_root, \"src\")\n",
    "utils_path = os.path.join(project_root, \"utils\")\n",
    "\n",
    "for p in [project_root, src_path, utils_path]:\n",
    "    if p not in sys.path:\n",
    "        sys.path.insert(0, p)\n",
    "\n",
    "print(\"Project root:\", project_root)\n",
    "print(\"src path:\", src_path)\n",
    "print(\"utils path:\", utils_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a01c2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import multivariate_normal\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from src.emissions import EmissionModel\n",
    "from src.transitions import TransitionModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65955d49",
   "metadata": {},
   "source": [
    "Stable log-sum-exp and forward using EmissionModel.logpdf  (check eqtn 11 in the paper):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7f8baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 1e-12\n",
    "\n",
    "def logsumexp_arr(a):\n",
    "    m = np.max(a)\n",
    "    return m + np.log(np.sum(np.exp(a-m)))\n",
    "\n",
    "def forward_loglik_subject(obs, times, init_probs, trans_mat, emission_model, g):\n",
    "\n",
    "    \"\"\"Compute the log-likelihood of a single subject given fixed\n",
    "    individual effects g using the forward algorithm in log-space for numerical stability.\n",
    "    Args:\n",
    "        obs (np.ndarray): Observations for the subject (T x D).\n",
    "        times (np.ndarray): Observation times for the subject (T,).\n",
    "        g: dict of random effects for the subject.\n",
    "        \"\"\"\n",
    "    T = len(obs)\n",
    "    n_states = 2\n",
    "    #initialize log alpha\n",
    "    log_alpha = np.zeros((T, n_states)) # log forward probabilities\n",
    "\n",
    "    for s in range(n_states):\n",
    "        log_em = emission_model.logpdf(obs[0], g, times[0], s)\n",
    "        log_alpha[0, s] = np.log(init_probs[s] + EPS) + log_em \n",
    "\n",
    "    #normalize in log-space by subtracting logsumexp\n",
    "    for t in range(1, T): #forward recursion\n",
    "        new_log_alpha = np.full(n_states, -np.inf)\n",
    "        for j in range(n_states):\n",
    "            #compute logsum over previous states\n",
    "            prev = log_alpha[t-1, :] + np.log(trans_mat[:, j] + EPS) \n",
    "            s_prev = logsumexp_arr(prev)\n",
    "            log_em = emission_model.logpdf(obs[t], g, times[t], j)\n",
    "            new_log_alpha[j] = s_prev + log_em #combine transition and emission log-probs\n",
    "        log_alpha[t, :] = new_log_alpha \n",
    "    \n",
    "    return logsumexp_arr(log_alpha[-1, :]) #marginal log-likelihood is logsumexp of final alphas  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99390563",
   "metadata": {},
   "source": [
    "Load simulated data and prep per-subject datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b70ffb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Week</th>\n",
       "      <th>State</th>\n",
       "      <th>FEV1</th>\n",
       "      <th>PRO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.651523</td>\n",
       "      <td>2.803810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.678107</td>\n",
       "      <td>3.123523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.348596</td>\n",
       "      <td>3.448288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.620862</td>\n",
       "      <td>2.832255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1.501967</td>\n",
       "      <td>3.267526</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  Week  State      FEV1       PRO\n",
       "0   1     0      0  1.651523  2.803810\n",
       "1   1     1      1  1.678107  3.123523\n",
       "2   1     2      1  1.348596  3.448288\n",
       "3   1     3      0  1.620862  2.832255\n",
       "4   1     4      1  1.501967  3.267526"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/simulated/ref_scenario.csv'\n",
    "assert os.path.exists(data_path), f\"Data file not found at {data_path}. Run 02_simulator.ipynb to generate it.\"\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac177910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, [1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subjects = df['ID'].unique()\n",
    "subj_data = {}\n",
    "\n",
    "for sid in subjects:\n",
    "    sub = df[df.ID ==sid].sort_values(\"Week\")\n",
    "    obs = sub[[\"FEV1\",\"PRO\"]].values\n",
    "    times = sub[\"Week\"].values.astype(float)\n",
    "    subj_data[int(sid)] = {'obs': obs, 'times': times}\n",
    "\n",
    "len(subj_data), list(subj_data.keys())[:5] # Check number of subjects and first 5 IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9156e064",
   "metadata": {},
   "source": [
    "Monte_Carlo marginal log-likelihood per subject\n",
    "\n",
    "-For each candidate population parameter set, for each subject we:\n",
    "\n",
    "-> draw K samples of g ~ N(0, x2_*) using the candidate x2 variances,\n",
    "\n",
    "-> compute forward log-likelihood for each g,\n",
    "\n",
    "-> average the likelihoods (in log-space using log-sum-exp) to approximate the marginal likelihood.\n",
    "\n",
    "Since this is slow for many subjects / large K, it's advisable to use small tests (e.g., K=30) before scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48e6df7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_g_matrix_em_params(n_samples, rng, x2_params):\n",
    "    \"\"\"\n",
    "    Draw n_samples of g according to x2 variances dict.\n",
    "    x2_params: dict with keys ['x2_FEV1R','x2_FEV1E','x2_PROR','x2_PROE']\n",
    "    Returns list of dicts length n_samples.\n",
    "    \"\"\"\n",
    "    s_FEV1R = np.sqrt(x2_params['x2_FEV1R'])  #standard deviations\n",
    "    s_FEV1E = np.sqrt(x2_params['x2_FEV1E'])\n",
    "    s_PROR = np.sqrt(x2_params['x2_PROR'])\n",
    "    s_PROE = np.sqrt(x2_params['x2_PROE'])\n",
    "    rng = np.random.default_rng(rng)\n",
    "    gs = []\n",
    "    for _ in range(n_samples): #draw n_samples\n",
    "        g = {\n",
    "            \"gFEV1R\": float(rng.normal(0.0, s_FEV1R)),\n",
    "            \"gFEV1E\": float(rng.normal(0.0, s_FEV1E)),\n",
    "            \"gPROR\":  float(rng.normal(0.0, s_PROR)),\n",
    "            \"gPROE\":  float(rng.normal(0.0, s_PROE))\n",
    "        }\n",
    "        gs.append(g)\n",
    "    return gs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0af08a72-6deb-490e-a03f-9262406f09d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.sampling_utils' from '/Users/user/Documents/Projects/Markov-Models/mHMM/utils/sampling_utils.py'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import utils.sampling_utils as su\n",
    "importlib.reload(su) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5f3d606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_subject_log_marginal(obs, times, init_probs, trans_mat, em_params, K=30, rng_seed=None):\n",
    "    \"\"\"Approximate the marginal log-likelihood of a subject\n",
    "    by Monte-Carlo integration over g.\n",
    "    Args:\n",
    "        obs (np.ndarray): Observations for the subject (T x D).\n",
    "        times (np.ndarray): Observation times for the subject (T,).\n",
    "        em_params: dict of emission parameters including variance components.\n",
    "        K (int): Number of Monte-Carlo samples.\"\"\"\n",
    "\n",
    "    #build emission model from em_params\n",
    "    em = EmissionModel(\n",
    "        hFEV1R=em_params['hFEV1R'],\n",
    "        hFEV1E=em_params['hFEV1E'],\n",
    "        x2_FEV1R=em_params['x2_FEV1R'],\n",
    "        x2_FEV1E=em_params['x2_FEV1E'],\n",
    "        hPROR=em_params['hPROR'],\n",
    "        hPROE=em_params['hPROE'],\n",
    "        x2_PROR=em_params['x2_PROR'],\n",
    "        x2_PROE=em_params['x2_PROE'],\n",
    "        r2_FEV1=em_params['r2_FEV1'],\n",
    "        r2_PRO=em_params['r2_PRO'],\n",
    "        qR=em_params['qR'],\n",
    "        qE=em_params['qE'],\n",
    "        PE=em_params['PE'],\n",
    "        PHL=em_params['PHL']\n",
    "    )\n",
    "\n",
    "    #sample K g's\n",
    "    x2_params = {k: em_params[k] for k in ['x2_FEV1R','x2_FEV1E','x2_PROR','x2_PROE']} \n",
    "    gs = sample_g_matrix_em_params(K, rng_seed, x2_params)\n",
    "    #compute log-likelihoods for each g\n",
    "    logls = []\n",
    "    for g in gs:\n",
    "        logl_g = forward_loglik_subject(obs, times, init_probs, trans_mat, em, g)\n",
    "        logls.append(logl_g)\n",
    "    #combine via log-mean-exp: log(1/k * sum exp(logl_g ))\n",
    "    logls = np.array(logls)\n",
    "    lm = logsumexp_arr(logls) - np.log(K)\n",
    "    return lm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7db573",
   "metadata": {},
   "source": [
    "## Full data negative log-likelihood to optimize\n",
    "\n",
    "We assemble a function neg_loglik(params) that maps a parameter vector to the negative log-likelihood across subjects using the Monte Carlo approximation.\n",
    "For clarity we will optimize a reduced parameter set in this notebook\n",
    "\n",
    "Examples of parameters we estimate here:\n",
    "\n",
    "-> hFEV1R, hFEV1E, hPROR, hPROE (modes)\n",
    "\n",
    "-> log of IIV variances: log_x2_FEV1R, log_x2_FEV1E, log_x2_PROR, log_x2_PROE\n",
    "\n",
    "-> log of residual variances: log_r2_FEV1, log_r2_PRO\n",
    "\n",
    "-> Fisher-transformed correlations btn the variables: atanh(qR), atanh(qE)\n",
    "\n",
    "-> hpRE and hpER (we optimize them on logit scale)\n",
    "\n",
    "For a quick test we only fit a subset (e.g., hFEV1R, hPROR, log residuals). The fixed_mask is for fixing parameters if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "971090a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def logit(p):\n",
    "    p = np.clip(p, EPS, 1.0 - EPS)\n",
    "    return np.log(p / (1.0 - p))\n",
    "# Parameter indexing helpers (order)\n",
    "param_names = [\n",
    "    'hFEV1R','hFEV1E','hPROR','hPROE',\n",
    "    'log_x2_FEV1R','log_x2_FEV1E','log_x2_PROR','log_x2_PROE',\n",
    "    'log_r2_FEV1','log_r2_PRO',\n",
    "    'atanh_qR','atanh_qE',\n",
    "    'logit_hpRE','logit_hpER'\n",
    "]\n",
    "\n",
    "def pack_params_from_vector(vec):\n",
    "    \"\"\"\n",
    "    Convert unconstrained vector to em_params dict and transition params.\n",
    "    `vec` must follow the param_names order.\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    idx = 0\n",
    "    d['hFEV1R'] = vec[idx]; idx+=1  #must be positive  \n",
    "    d['hFEV1E'] = vec[idx]; idx+=1\n",
    "    d['hPROR']  = vec[idx]; idx+=1\n",
    "    d['hPROE']  = vec[idx]; idx+=1\n",
    "\n",
    "    d['x2_FEV1R'] = np.exp(vec[idx]); idx+=1\n",
    "    d['x2_FEV1E'] = np.exp(vec[idx]); idx+=1\n",
    "    d['x2_PROR']  = np.exp(vec[idx]); idx+=1\n",
    "    d['x2_PROE']  = np.exp(vec[idx]); idx+=1\n",
    "\n",
    "    d['r2_FEV1'] = np.exp(vec[idx]); idx+=1\n",
    "    d['r2_PRO']  = np.exp(vec[idx]); idx+=1\n",
    "\n",
    "    d['qR'] = np.tanh(vec[idx]); idx+=1\n",
    "    d['qE'] = np.tanh(vec[idx]); idx+=1\n",
    "\n",
    "    d['hpRE'] = logistic(vec[idx]); idx+=1\n",
    "    d['hpER'] = logistic(vec[idx]); idx+=1\n",
    "\n",
    "    # placeholders for PE, PHL (fixed)\n",
    "    d['PE'] = 0.2\n",
    "    d['PHL'] = 10.0\n",
    "    return d\n",
    "\n",
    "# default initial vector set to values close to simulation\n",
    "init_vec = np.array([\n",
    "    3.0, 0.5,  # hFEV1R, hFEV1E\n",
    "    2.5, 0.5,  # hPROR, hPROE\n",
    "    np.log(0.03), np.log(0.03), np.log(0.09), np.log(0.09),  # log x2\n",
    "    np.log(0.015), np.log(0.05),  # log r2\n",
    "    np.arctanh(-0.33), np.arctanh(-0.33),  # atanh q\n",
    "    logit(0.1), logit(0.3)  # logit hpRE, hpER\n",
    "])\n",
    "\n",
    "# Choose a subset to optimize for a quick demo (e.g., first 10 params)\n",
    "free_idx = list(range(len(init_vec)))  # by default free all\n",
    "# for a faster demo you might limit:\n",
    "# free_idx = list(range(10))  # fit only the first 10 parameters\n",
    "\n",
    "def neg_loglik_mc(vec, subj_data_local, init_probs, trans_mat_fn, K=30, rng_seed=42):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood across all subjects approximated by MC integration.\n",
    "    trans_mat_fn: function that given em_params returns transition matrix (we keep hpRE/hpER in params)\n",
    "    \"\"\"\n",
    "    em_params = pack_params_from_vector(vec)\n",
    "    # build trans_mat from hpRE/hpER (no gpRE/gpER here)\n",
    "    tm = TransitionModel(hpRE=em_params['hpRE'],gpRE=0.0, hpER=em_params['hpER'], gpER=0.0) \n",
    "    trans_mat = tm.transition_matrix()\n",
    "\n",
    "    total_logl = 0.0\n",
    "    rng_seq = np.random.SeedSequence(rng_seed)\n",
    "    # Loop subjects (consider batching / parallelization later)\n",
    "    for i, sid in enumerate(subj_data_local.keys()):\n",
    "        d = subj_data_local[sid]\n",
    "        lm = approx_subject_log_marginal(\n",
    "            d['obs'], d['times'],\n",
    "            init_probs, trans_mat, em_params, K=K, rng_seed=rng_seq.spawn(1)[0].entropy\n",
    "        )\n",
    "        total_logl += lm\n",
    "    return -float(total_logl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc46b11",
   "metadata": {},
   "source": [
    "Quick test of the fn neg_loglik with a small subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0df31961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neg log-likelihood (5 subjects, K=20): 464.740 computed in 0.5 seconds\n"
     ]
    }
   ],
   "source": [
    "test_subjects = {k: subj_data[k] for k in list(subj_data.keys())[:5]}  # first 5 subjects for quick test\n",
    "init_probs = np.array([0.9, 0.1])  \n",
    "\n",
    "t0 = time.time()\n",
    "nl = neg_loglik_mc(init_vec, test_subjects, init_probs, None, K=20, rng_seed=1)\n",
    "t1 = time.time()\n",
    "print(f\"Neg log-likelihood (5 subjects, K=20): {nl:.3f} computed in {t1 - t0:.1f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731d9733",
   "metadata": {},
   "source": [
    "# Optimization demo\n",
    "\n",
    "-> We minimize the negative log-likelihood of our mHMM model to find the params that maximizie the likelihood of our observed data.\n",
    "\n",
    "-> We use L-BFGS-B algorithm - gradient-based optimization algorithm that efficiently handles large numbers of parameters and parameter bounds (e.g. variances > 0). More stable than basic gradient-descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "565cd611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimization result:\n",
      "  message: STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "  success: False\n",
      "   status: 1\n",
      "      fun: 20.98552719714403\n",
      "        x: [ 1.816e+00  2.048e-01 ... -3.983e+00 -1.158e+00]\n",
      "      nit: 50\n",
      "      jac: [-3.146e+01  1.319e+01 ...  6.740e-01  3.392e-01]\n",
      "     nfev: 975\n",
      "     njev: 65\n",
      " hess_inv: <14x14 LbfgsInvHessProduct with dtype=float64>\n"
     ]
    }
   ],
   "source": [
    "subject_subset = {k: subj_data[k] for k in list(subj_data.keys())[:10]}  \n",
    "\n",
    "#show progress \n",
    "history = []\n",
    "def cb(xk):\n",
    "    history.append(xk.copy())\n",
    "    print('.', end='') \n",
    "\n",
    "res = minimize(\n",
    "    fun = lambda v: neg_loglik_mc(v, subject_subset, init_probs, None, K=30, rng_seed=1),\n",
    "    x0=init_vec,\n",
    "    method='L-BFGS-B',\n",
    "    options={'disp': True, 'maxiter': 50}\n",
    ")\n",
    "\n",
    "print(\"\\nOptimization result:\")\n",
    "print(res) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff39f9-dfdb-4883-9e43-eb4486aa0cbe",
   "metadata": {},
   "source": [
    "### Intepretation:\n",
    "> Our L-BFGS-B hit the max iter limit (50) and stopped. The \"false\" is bc the optimizer didn't find a stopping condition.\n",
    "> The \"fun\" is the negative log-likelihood value at the final param vector, which is in a normal range for the 10 subjects\n",
    "> x is our em_params list\n",
    "> nfev represents 975 negative-log likelihood evaluations and \"nit\" is for 50 optimizer steps\n",
    "> We will increase the optimization quality later to 200 for maxiter and k=100. This was for testing our pipeline only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecea832",
   "metadata": {},
   "source": [
    "# Interpret & decode Viterbi using a point estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b7979dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if res.success:\n",
    "    fitted_params = pack_params_from_vector(res.x)\n",
    "    tm_hat = TransitionModel(hpRE=fitted_params['hpRE'], hpER=fitted_params['hpER'])\n",
    "    trans_mat_hat = tm_hat.transition_matrix()\n",
    "    em_hat = EmissionModel(\n",
    "        hFEV1R=fitted_params['hFEV1R'], hFEV1E=fitted_params['hFEV1E'],\n",
    "        x2_FEV1R=fitted_params['x2_FEV1R'], x2_FEV1E=fitted_params['x2_FEV1E'],\n",
    "        hPROR=fitted_params['hPROR'], hPROE=fitted_params['hPROE'],\n",
    "        x2_PROR=fitted_params['x2_PROR'], x2_PROE=fitted_params['x2_PROE'],\n",
    "        r2_FEV1=fitted_params['r2_FEV1'], r2_PRO=fitted_params['r2_PRO'],\n",
    "        qR=fitted_params['qR'], qE=fitted_params['qE'],\n",
    "        PE=fitted_params['PE'], PHL=fitted_params['PHL']\n",
    "    )\n",
    "\n",
    "    from mHMM.src.mhmm_forward import viterbi  \n",
    "    # Naive g=0\n",
    "    g0 = {\"gFEV1R\":0.0,\"gFEV1E\":0.0,\"gPROR\":0.0,\"gPROE\":0.0}\n",
    "    sample_sid = list(subject_subset.keys())[0]\n",
    "    sdata = subject_subset[sample_sid]\n",
    "    states_decoded = viterbi(sdata['obs'], np.array([0.9,0.1]), trans_mat_hat, em_hat, g0, sdata['times'])\n",
    "    print(\"Decoded states (naive g=0):\", states_decoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a0b92d",
   "metadata": {},
   "source": [
    "Run CmdStanPy Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf57ae6",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this notebook, we did deterministic Maximum Likelihood Estimation (MLE) for the mixed Hidden Markov Model (mHMM) using Python, without Stan. The main steps were:\n",
    "\n",
    "1. Data Preparation\n",
    "\n",
    "Loaded the simulated reference dataset from 02_simulator.ipynb and reshaped it into the format required for likelihood evaluation:\n",
    "\n",
    "Flattened the longitudinal outcomes (FEV1, PRO)\n",
    "\n",
    "Generated time, subject start indices, and lengths\n",
    "\n",
    "Constructed a clean stan_data-like structure for use in Python\n",
    "\n",
    "This ensures each subject‚Äôs complete time series can be passed to the likelihood function.\n",
    "\n",
    "2. Monte Carlo Marginal Likelihood\n",
    "\n",
    "Implemented the subject-specific marginal likelihood under random effects by:\n",
    "\n",
    "Sampling individual random effects > ùëî using the population variance parameters\n",
    "\n",
    "Evaluating state‚Äêconditional emission probabilities\n",
    "\n",
    "Propagating forward probabilities through the transition model\n",
    "\n",
    "Averaging over Monte Carlo draws to approximate the integral\n",
    "\n",
    "Aggregating contributions across subjects\n",
    "\n",
    "This forms the full log-likelihood for the mHMM.\n",
    "\n",
    "3. Maximum Likelihood Estimation (L-BFGS-B)\n",
    "\n",
    "Set up an unconstrained parameter vector, applied transforms (logit, log, atanh) for valid parameter domains, and optimized the negative log-likelihood using:\n",
    "\n",
    "L-BFGS-B\n",
    "\n",
    "A limited number of iterations (for testing pipeline correctness)\n",
    "\n",
    "While the test run stops at the iteration limit, it confirms that:\n",
    "\n",
    "The likelihood is well-defined\n",
    "\n",
    "Gradients can be computed\n",
    "\n",
    "Optimization moves toward a plausible region\n",
    "\n",
    "4. Recovering Model Parameters\n",
    "\n",
    "After optimization, we unpacked the fitted parameter vector into:\n",
    "\n",
    "Emission parameters\n",
    "\n",
    "Random-effect variances\n",
    "\n",
    "Transition parameters\n",
    "\n",
    "Placebo/time effect parameters\n",
    "\n",
    "This produces a complete set of point estimates for all population-level quantities in the mHMM.\n",
    "\n",
    "5. Viterbi Decoding (Optional Demonstration)\n",
    "\n",
    "Using the estimated parameters, we decoded the most likely latent state sequence for one sample subject using the Viterbi algorithm.\n",
    "This provides an interpretable check of how the estimated model classifies latent responder / exacerbation states.\n",
    "\n",
    "## Overall Purpose of This Notebook\n",
    "\n",
    "This notebook implements the frequentist MLE pipeline for the mixed HMM:\n",
    "\n",
    "Deterministic\n",
    "\n",
    "Fast to test\n",
    "\n",
    "Useful for debugging parameterizations and data flow\n",
    "\n",
    "Matches the \"MC-MLE\" approach used as a baseline before SSE/SAEM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ce53d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5f44c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mhmm_env)\n",
   "language": "python",
   "name": "mhmm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
