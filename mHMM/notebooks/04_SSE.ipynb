{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99fa35c2",
   "metadata": {},
   "source": [
    "# Stochastic Simulation Estimation\n",
    "\n",
    "- We create a pipeline to automate repeated simulation under known parameters\n",
    "- We automate model estimation, using Stan or Monte-Carlo MLE\n",
    "- Collect parameter estimates\n",
    "- Compute bias, RMSE and coverage across replicates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39015d5",
   "metadata": {},
   "source": [
    "# Estimation Framework Choice — Stan vs NONMEM + SAEM\n",
    "\n",
    "In the original study, model estimation was performed in NONMEM using the **Stochastic Approximation Expectation–Maximization (SAEM)** algorithm.  \n",
    "To enable full reproducibility in an open-source environment, we replaced SAEM with **Bayesian inference implemented in Stan (via CmdStanPy)**.\n",
    "\n",
    "Stan integrates over both **random effects** and **hidden states** using Hamiltonian Monte Carlo (HMC) sampling, producing posterior distributions for all parameters.  \n",
    "This approach is mathematically equivalent to maximizing the marginal likelihood (MLE) under SAEM, but provides richer uncertainty quantification and does not depend on proprietary software(such as NONMEM).\n",
    "\n",
    "Accordingly, all simulation–estimation (SSE) results in this notebook are based on **Stan-derived posterior means and credible intervals** as open-source analogues to SAEM estimates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818e9510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "import time \n",
    "\n",
    "import datetime\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "#for parallel processing\n",
    "from joblib import Parallel, delayed\n",
    "from cmdstanpy import CmdStanModel\n",
    "import scipy.optimize as opt\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, str(Path(\"...\").resolve()))\n",
    "from mHMM.src.emissions import EmissionModel\n",
    "from mHMM.src.transitions import TransitionModel\n",
    "\n",
    "#SSE config\n",
    "BASE_DIR = Path('...')\n",
    "DATA_DIR = BASE_DIR / 'data' / 'simulated'\n",
    "RESULTS_DIR = BASE_DIR / 'data' / 'results' / 'SSE'\n",
    "STAN_FILE = BASE_DIR / 'models' / 'stan' / 'mhmm_model.stan' \n",
    "\n",
    "os.makedirs(RESULTS_DIR, exist_ok = True)\n",
    "\n",
    "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "logfile = RESULTS_DIR / f\"sse_run_{timestamp}.log\"\n",
    "logging.basicConfig(\n",
    "    filename= str(logfile),\n",
    "    filemode = 'a',\n",
    "    format = '%(asctime)s %(levelname)s: %(message)s',\n",
    "    level = logging.INFO\n",
    ")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "logging.INFO('SSE runner initialized') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82ae92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import SeedSequence, default_rng\n",
    "\n",
    "def rng_from_seed(seed):\n",
    "    \"\"\"Return a np generator deterministically from seed (int or None)\"\"\"\n",
    "    ss = SeedSequence(seed if seed is not None else int(time.time()*1e6) & 0xFFFFFFFF) \n",
    "    return default_rng  \n",
    "\n",
    "def spawn_seed(seed):\n",
    "    \"\"\"Return a new entropy int from a SeedSequence-based stream for subprocesses\"\"\"\n",
    "    ss = SeedSequence(seed)\n",
    "    child = ss.spawn(1)[0] \n",
    "    return int(child.entropy)  #convert a number to int or return 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9d79fe",
   "metadata": {},
   "source": [
    "Simulation Wrapper (deterministic, Our reference scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b006a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_reference_dataset(seed, N_subj=100, T_weeks =60, init_probs=(0.9, 0.1),\n",
    "                               trans_params=None, em_params=None): \n",
    "    \"\"\"Simulates a full dataset according to the paper's ref scenario. Return\n",
    "    a pandas df with columns: ID, Week, State, FEV1, PRO\"\"\"\n",
    "    rng = rng_from_seed(seed)\n",
    "    if trans_params is None:\n",
    "        trans_params = {'hPRE':0.1,'hPER':0.3, 'gPRE':0.0,'gPER':0.0, \n",
    "                        'trt':0, 'slp':0 }\n",
    "    if em_params is None:\n",
    "        em_params = dict(\n",
    "            hFEV1R=3.0, hFEV1E=0.5,\n",
    "            x2_FEV1R=0.03, x2_FEV1E=0.03,\n",
    "            hPROR=2.5, hPROE=0.5,\n",
    "            x2_PROR=0.09, x2_PROE=0.09,\n",
    "            r2_FEV1=0.015, r2_PRO=0.05,\n",
    "            qR=-0.33, qE=-0.33,\n",
    "            PE=0.2, PHL=10.0\n",
    "        )\n",
    "\n",
    "    em = EmissionModel(**em_params) \n",
    "    tm = TransitionModel(hpRE=trans_params['hPRE'], hpER=trans_params['hPER'],\n",
    "                         gpRE=trans_params.get('gPRE',0.0), gpER=trans_params.get('gPER',0.0),\n",
    "                         trt=trans_params.get('trt',0), slp=trans_params.get('slp',0)) \n",
    "    \n",
    "    trans_mat = tm.transition_matrix()\n",
    "    times = np.arange(T_weeks)\n",
    "\n",
    "    rows = []\n",
    "    for sid in range(1,N_subj+1):\n",
    "        g = em.sample_individual_effects(rng=rng)\n",
    "        #simulate states\n",
    "        states = np.zeros(T_weeks, dtype=int)\n",
    "        states[0] = rng.choice([0,1], p=init_probs)\n",
    "        for t in range(1, T_weeks):\n",
    "            states[t] = rng.choice([0,1], p=trans_mat[states[t-1], :])\n",
    "\n",
    "        #simulate observations\n",
    "        for t in range(T_weeks):\n",
    "            mu = np.array([em.individual_fev1(g, states[t]), em.individual_pro(g, t, states[t])])\n",
    "            cov = em.emission_cov(states[t])\n",
    "            y = rng.multivariate_normal(mu, cov)\n",
    "            rows.append({'ID': sid, 'Week': int(t+1), 'State': int(states[t]), 'FEV1': float(y[0]), 'PRO': float(y[1])})\n",
    "\n",
    "    df = pd.DataFrame(rows) \n",
    "    return df "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41234681",
   "metadata": {},
   "source": [
    "# Stan Data Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea79ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stan_data(df, init_probs=(0.9,0.1),trt_slp_val=0.0):\n",
    "\n",
    "    df = df.sort_values(['ID', 'Week']).reset_index(drop=True)\n",
    "    subjects = df[\"ID\"].unique().tolist()\n",
    "    N = len(subjects)\n",
    "    y1_list, y2_list, time_list = [], [], []\n",
    "    subj_start, subj_len, trt_slp = [], [], []\n",
    "\n",
    "    pos = 0 #position in flat arrays\n",
    "    for sid in subjects:\n",
    "        sub=df[df[\"ID\"] == sid].sort_values(\"Week\")\n",
    "        subj_start.append(pos + 1)  #1-based index for Stan\n",
    "        L = sub.shape[0] \n",
    "        subj_len.append(L)\n",
    "        pos +=L\n",
    "        y1_list.extend(sub['FEV1'].astype(float).tolist())\n",
    "        y2_list.extend(sub['PRO'].astype(float).tolist())\n",
    "        time_list.extend(sub[\"Week\"].astype(float).tolist())  \n",
    "        trt_slp.append(float(trt_slp_val)) \n",
    "\n",
    "    stan_data = {\n",
    "        'N':N,\n",
    "        'T_max': max(subj_len),\n",
    "        'total_obs':len(y1_list),\n",
    "        'subj_start': subj_start,\n",
    "        'subj_len': subj_len,\n",
    "        'y1_flat':y1_list,\n",
    "        'y2_flat': y2_list,\n",
    "        'time_flat': time_list,\n",
    "        'init_prob': list(init_probs),\n",
    "        'trt_slp': trt_slp\n",
    "\n",
    "    }\n",
    "    return stan_data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d4e93b",
   "metadata": {},
   "source": [
    "Fit Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3cf8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to compile the model once:\n",
    "\n",
    "assert STAN_FILE.exists(), f\"Stan file not found at {STAN_FILE}\"\n",
    "stan_model = CmdStanModel(stan_file=str(STAN_FILE))\n",
    "\n",
    "def fit_stan_for_df(df, out_dir, test_mode=True, chains=2, seed=12345, adapt_delta=0.85, threads=1):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    stan_data = build_stan_data(df)\n",
    "    #If test_mode, reduce iter/chain\n",
    "    iter_warmup = 400 if test_mode else 1000\n",
    "    iter_sampling = 400 if test_mode else 1000\n",
    "    chains = 2 if test_mode else chains \n",
    "\n",
    "    try: \n",
    "        fit = stan_model.sample(\n",
    "            data=stan_data,\n",
    "            chains=chains,\n",
    "            iter_warmup=iter_warmup,\n",
    "            iter_sampling = iter_sampling,\n",
    "            adapt_delta=adapt_delta,\n",
    "            seed=seed,\n",
    "            show_progress=False,\n",
    "            threads=threads\n",
    "        )\n",
    "\n",
    "        ts = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        pkl = out_dir / f\"stan_fit_{ts}.pkl\"\n",
    "        with open(pkl, \"wb\") as f:\n",
    "            pickle.dump(fit, f)\n",
    "\n",
    "            fit.save_csvfiles(dir=str(out_dir))\n",
    "            logging.info(f\"Stan fit saved: {pkl}\")\n",
    "            return pkl \n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.exception(\"Stan fit failed\")\n",
    "        raise \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3168e",
   "metadata": {},
   "source": [
    "# Extract posterior summaries (means & 95% Cl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_stan_fit(pickle_path):\n",
    "\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        fit = pickle.load(f)\n",
    "    \n",
    "    df = fit.draws_pd()\n",
    "    #transform derived vars\n",
    "    df['hpRE'] = 1/(1+np.exp(-df[\"logit_hpRE\"])) if \"logit_hpRE\" in df.columns else None\n",
    "    df['hpER'] = 1/(1+np.exp(-df[\"logit_hpER\"])) if \"logit_hpER\" in df.columns else None\n",
    "    if \"atanh_qR\" in df.columns:\n",
    "        df[\"qR\"] = np.tanh(df[\"atanh_qR\"]) \n",
    "    if \"atanh_qE\" in df.columns:\n",
    "        df['qE'] = np.tanh(df['qE'])\n",
    "\n",
    "    \n",
    "    keys = [\"hFEV1R\",\"hFEV1E\",\"hPROR\",\"hPROE\",\"r2_FEV1\",\"r2_PRO\",\"qR\",\"qE\",\"hpRE\",\"hpER\",\"PE\",\"PHL\"]\n",
    "    result = {}\n",
    "    for k in keys:\n",
    "        if k in df.columns:\n",
    "            arr = df[k].dropna().values\n",
    "            result[k+\"_mean\"] = float(np.mean(arr))\n",
    "            result[k+\"_sd\"] = float(np.std(arr, ddof=1))\n",
    "            result[k+\"_2.5%\"] = float(np.percentile(arr, 2.5))\n",
    "            result[k+\"_97.5%\"] = float(np.percentile(arr, 97.5))\n",
    "\n",
    "    return result \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4be237b",
   "metadata": {},
   "source": [
    "Single Pipeline : Simulate --> Fit --> Summarize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcff709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_replicate(rep_idx, seed_base, out_root, test_mode=True, use_stan=True, stan_threads=1):\n",
    "    \"\"\"\n",
    "    -Simulaye dataset\n",
    "    -fit with stan\n",
    "    -return dict with metadata and summary, save files\"\"\"\n",
    "\n",
    "    start_time=time.time()\n",
    "    rep_out = Path(out_root) / f\"rep_{rep_idx:04d}\"\n",
    "    rep_out.mkdir(parents=True, exist_ok=True)\n",
    "    seed = int((seed_base + rep_idx) & 0x7FFFFFFF)\n",
    "    log_prefix = f\"rep_{rep_idx}\"\n",
    "\n",
    "    try:\n",
    "        #simulate\n",
    "        df = simulate_reference_dataset(seed, N_subj=100, T_weeks=60)\n",
    "        #save \n",
    "        csvp = rep_out / \"simulated.csv\"\n",
    "        df.to_csv(csvp, index=False)\n",
    "\n",
    "        if use_stan:\n",
    "            pkl_path = fit_stan_for_df(df, out_dir=rep_out, test_mode = test_mode,seed=seed, threads=stan_threads)\n",
    "            summary = summarize_stan_fit(pkl_path)\n",
    "        else:\n",
    "            summary = {\"status\": \"MLE_not_implemented\"}\n",
    "\n",
    "        elapsed = time.time() - start_time \n",
    "        meta = {\n",
    "            \"rep\": rep_idx,\n",
    "            \"seed\": seed,\n",
    "            \"elapsed_s\": elapsed,\n",
    "            \"status\": \"ok\" \n",
    "        }\n",
    "        meta.update(summary)\n",
    "\n",
    "        with open(rep_out / \"summary.pkl\", \"wb\") as f:\n",
    "            pickle.dump(meta, f)  #save\n",
    "        return meta\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Replicate {rep_idx} failed\")\n",
    "\n",
    "        with open(rep_out / \"error.txt\", \"w\") as f:\n",
    "            f.write(str(e))\n",
    "        return {\"rep\": rep_idx, \"status\":\"failed\", \"error\": str(e)} \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddf296e",
   "metadata": {},
   "source": [
    "# Parallel SSE executor (joblib) and aggregator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sse(n_reps=50, seed_base=1234, out_root=RESULTS_DIR, test_mode=True,\n",
    "             use_stan=True, n_jobs=2, stan_threads=1):\n",
    "    \n",
    "    logging.info(f\"Starting SSE: n_reps={n_reps}, test_mode={test_mode}, use_stan={use_stan}\")\n",
    "    start = time.time()\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(run_single_replicate)(i+1, seed_base, out_root, test_mode, use_stan, stan_threads)\n",
    "                                      for i in tqdm(range(n_reps), desc=\"SSE replicates\")) \n",
    "    df_res = pd.DataFrame(results)\n",
    "    csv_out = Path(out_root) / f\"sse_summary_all_{timestamp}.csv\"\n",
    "    df_res.to_csv(csv_out, index=False)\n",
    "    logging.info(f\"SSE finished in {(time.time() - start)/60: .2f} min. Results saved to {csv_out}\" )\n",
    "    return df_res \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TEST CELL\n",
    "df_results = run_sse(n_reps=4, seed_base=999, out_root=RESULTS_DIR, test_mode=True, use_stan=True, n_jobs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68b0244",
   "metadata": {},
   "source": [
    "Post-Processing: bias, RMSE, coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ef26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sse_metrics(summary_df, reference_params):\n",
    "    #taking only successful replicates\n",
    "\n",
    "    ok = summary_df[summary_df[\"status\"] == \"ok\"].copy()\n",
    "    metrics = []\n",
    "    n_ok = ok.shape[0] \n",
    "    logging.info(f\"Computing metrics on {n_ok} succesful replicates out of {len(summary_df)}\")\n",
    "\n",
    "    for key, true_val in reference_params.items():\n",
    "        mean_key = f\"{key}_mean\"\n",
    "        lower_key = f\"{key}_2.5%\"\n",
    "        upper_key = f\"{key}_97.5%\"\n",
    "\n",
    "        if mean_key not in ok.columns:\n",
    "            logging.warning(f\"{mean_key} not in summaries; skipping\")\n",
    "            continue \n",
    "\n",
    "        ests = ok[mean_key].astype(float)\n",
    "        bias = (ests - true_val).mean() \n",
    "        rmse = np.sqrt(((ests - true_val)**2).mean()) \n",
    "\n",
    "        if lower_key in ok.columns and upper_key in ok.columns:\n",
    "            cov = ((ok[lower_key] <= true_val) & ok[upper_key] >= true_val).mean()\n",
    "        else:\n",
    "            cov= np.nan\n",
    "\n",
    "        metrics[key] = {\"Bias\": float(bias), \"RMSE\": float(rmse), \"Coverage\": float(cov), \"N\": int(n_ok)}\n",
    "\n",
    "    return pd.DataFrame(metrics).T "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32617234",
   "metadata": {},
   "source": [
    "## Recommended workflow to run SSE\n",
    "\n",
    "1. **Smoke test**: run `run_sse(n_reps=4, test_mode=True, n_jobs=2)` to verify the pipeline.\n",
    "2. **Small experiment**: run `n_reps=20` with `test_mode=True` to validate behavior and resource use.\n",
    "3. **Full SSE**: run `n_reps=100` (or 200) with `test_mode=False` and `n_jobs` set to the number of CPU cores you can allocate.\n",
    "   - Use `stan_threads>=1` and ensure your machine has enough memory (Stan uses memory per chain).\n",
    "4. **Parallelization notes**:\n",
    "   - `n_jobs` controls number of concurrent replicates. Each replicate launches CmdStan and will use `stan_threads` threads.\n",
    "   - On HPC, prefer job-array or dask for large SSE jobs to avoid too many simultaneous CmdStan compiles/samples.\n",
    "5. **Caching**: results are saved per-replicate in `data/results/SSE/rep_XXXX`, so re-runs resume cleanly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9807c03d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489a19a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e1236d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mhmm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
